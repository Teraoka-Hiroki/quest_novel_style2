{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNRdUCa0bHd0nXjtvurIPFn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 小説の文章スタイルを分析する"],"metadata":{"id":"IAbTYWXnoAMo"}},{"cell_type":"markdown","source":["ライブラリのインストール"],"metadata":{"id":"IGYGO0avoPrl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bR5qUqCBdMZY"},"outputs":[],"source":["!apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n","!pip install unidic-lite --quiet\n","!pip install mecab-python3 --quiet\n","!pip install Levenshtein --quiet\n","#!pip install nltk --quiet\n"]},{"cell_type":"markdown","source":["ドライブのマウント\n"],"metadata":{"id":"iXZT45v7oaCL"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"h6nqPCNifTuI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["テキストファイルの文章から'動詞', '形容詞', '副詞', '連体詞', '接続詞'の単語を抜き出す。\n","\n","元のテキストファイル：input_folder  \n","作成したテキストファイル：output_folder\n"],"metadata":{"id":"pFWQ8-ryoleV"}},{"cell_type":"code","source":["import MeCab\n","import os\n","import re\n","import chardet\n","\n","# ドライブ内のファイルが格納されたフォルダのパス\n","input_folder = '/content/drive/MyDrive/Mago_Serise/mago121(sentence_style)/novel_style/C_00/'\n","output_folder = '/content/drive/MyDrive/Mago_Serise/mago121(sentence_style)/novel_style/C_01'\n","\n","# MeCabの初期化\n","tagger = MeCab.Tagger()\n","\n","# 入力フォルダ内のすべてのテキストファイルを処理\n","for filename in os.listdir(input_folder):\n","    if filename.endswith(\".txt\"):\n","        # テキストファイルのパス\n","        input_file_path = os.path.join(input_folder, filename)\n","\n","        # テキストファイルから文章を読み込む（エンコーディングを自動検出）\n","        with open(input_file_path, 'rb') as file:\n","            rawdata = file.read()\n","        encoding = chardet.detect(rawdata)['encoding']\n","        text = rawdata.decode(encoding)\n","\n","        text = re.sub(\"《[^》]+》\", \"\", text)  # ルビの削除\n","        text = re.sub(\"［[^］]+］\", \"\", text)  # 読みの注意の削除\n","        text = re.sub(\"【([^】]*)】\", \"\", text)  # 読みの注意の削除\n","        text = re.sub(\"[｜ 　]\", \"\", text)  # | と全角半角スペースの削除\n","\n","        # 形態素解析を行い、品詞を抽出して新しい文章を生成\n","        new_sentence = []\n","        node = tagger.parseToNode(text)\n","        while node:\n","            features = node.feature.split(',')\n","            if features[0] in ['動詞' '形容詞', '副詞', '連体詞', '接続詞']:\n","                new_sentence.append(node.surface)\n","            node = node.next\n","\n","        # 単語を一文字分のスペースで区切って結合\n","        new_sentence_str = '　'.join(new_sentence)\n","\n","        # 出力ファイルのパス\n","        output_file_path = os.path.join(output_folder, filename)\n","\n","        # 新しい文章をテキストファイルに書き出す\n","        with open(output_file_path, 'w', encoding='utf-8') as file:\n","            file.write(new_sentence_str)\n"],"metadata":{"id":"kclgY4b6OpqX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["単語の出現頻度表を作成\n"],"metadata":{"id":"TqUfJoxpt2Gd"}},{"cell_type":"code","source":["import os\n","import MeCab\n","import pandas as pd\n","from collections import Counter\n","\n","# ドライブ内のファイルが格納されたフォルダのパス\n","folder_path = '/content/drive/MyDrive/Mago_Serise/mago121(sentence_style)/novel_style/C_01'\n","\n","\n","# MeCabの初期化\n","m = MeCab.Tagger(\"-Owakati\")\n","\n","# 形態素と頻度を格納する辞書\n","word_freq_dict = {}\n","\n","# ファイルごとに形態素解析と単語の頻度を計算\n","file_list = os.listdir(folder_path)\n","\n","# すべての単語を含むリストを作成\n","unique_words = []\n","\n","for file_name in file_list:\n","    file_path = os.path.join(folder_path, file_name)\n","\n","    with open(file_path, 'rb') as file:\n","        text = file.read().decode('utf-8', errors='replace')  # 'utf-8'に変更\n","\n","    # 形態素解析して単語リストを取得\n","    parsed_text = m.parse(text)\n","    words = parsed_text.split()\n","\n","    # 形態素の頻度を計算\n","    word_freq = Counter(words)\n","\n","    # すべての単語をリストに追加\n","    unique_words.extend(word_freq.keys())\n","\n","    # 辞書に追加\n","    word_freq_dict[file_name] = word_freq\n","\n","# 一意の単語リストを作成\n","unique_words = list(set(unique_words))\n","\n","# 各ファイルの単語頻度を持つデータフレームを作成\n","df = pd.DataFrame(word_freq_dict).fillna(0)\n","\n","# 一意の単語リストを持つデータフレームを作成\n","word_df = pd.DataFrame(index=unique_words)\n","\n","# 一意の単語リストを持つデータフレームとファイルごとの単語頻度のデータフレームを結合\n","result_df = word_df.join(df)\n","\n","# NaNを0に置換\n","result_df = result_df.fillna(0)\n","\n","# 結果を表示\n","print(result_df)\n"],"metadata":{"id":"4DkLC8-zvbIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 結果をCSVファイルとして保存\n","result_path = '/content/drive/MyDrive/Mago_Serise/mago121(sentence_style)/novel_style/result' #パスを指定\n","df.to_csv(result_path + '/adjectives_adverbs_freq.csv', encoding='utf-8')\n"],"metadata":{"id":"ZYz0x5K95eoH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["多次元尺度法で表示\n"],"metadata":{"id":"jgKMTYkXuoH7"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import MDS\n","from sklearn.metrics import euclidean_distances\n","from Levenshtein import distance as levenshtein_distance\n","\n","# CSVファイルからデータを読み込む\n","data = pd.read_csv(result_path + '/adjectives_adverbs_freq.csv')\n","\n","# 1列目を除いた名前のリストを取得\n","names = data.columns.tolist()[1:]\n","\n","# 名前の類似度に基づいて距離行列を計算\n","distances = np.zeros((len(names), len(names)))\n","for i in range(len(names)):\n","    for j in range(len(names)):\n","        distances[i, j] = levenshtein_distance(names[i], names[j])\n","\n","# 多次元尺度法を適用して2次元の配置を取得\n","mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n","pos = mds.fit_transform(distances)\n","\n","# 結果をプロット\n","plt.figure(figsize=(10, 8))\n","plt.scatter(pos[:, 0], pos[:, 1], color='y', s=100)\n","\n","# 各点に名前を表示\n","for i, txt in enumerate(names):\n","    plt.annotate(txt, (pos[i, 0], pos[i, 1]))\n","\n","plt.title('Multidimensional Scaling Plot of Novels')\n","plt.xlabel('Dimension 1')\n","plt.ylabel('Dimension 2')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"Hru6-Za8OURl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZaBi2FBVOVBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KmewcQYuOVLb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iGuh1OZSOVVK"},"execution_count":null,"outputs":[]}]}